# 設備保全シナリオ比較分析と教訓

**日付**: 2025年12月21日  
**実験**: 3つの保全シナリオで1000エピソード訓練  
**環境**: ボイラー(40t) 温度センサー（2x2 Markov CBM）

---

## 📊 実験概要

### 目的
異なるリスク-コストバランスを持つ3つの保全戦略を比較し、最適な設備保全方針を特定する。

### 実験設定

**共通パラメータ:**
- Episodes: 1000
- Parallel Environments: 16
- Horizon: 100 steps
- Gamma: 0.95
- Algorithm: QR-DQN (51 quantiles)
- Optimizations: PER, N-step(3), AMP, Noisy Networks

**Actions:**
- DoNothing: コスト 0
- Repair: コスト 3
- Replace: コスト 15

**States:**
- Normal (9.4%): 正常温度範囲 [13.0, 40.0]°C
- Anomalous (90.6%): 異常温度範囲

**Transition Matrix (DoNothing):**
```
            Normal  Anomalous
Normal      0.2948    0.7052
Anomalous   0.0731    0.9269
```

→ 高い異常状態持続性 (92.7%)

---

## 🔬 テスト対象シナリオ

### 1. 安全重視 (Safety First)
```python
Risk Weight: 1.0
Cost Lambda: 0.05
```

**コンセプト**: 設備停止を回避し、積極的に保全  
**Replace実効コスト**: 15 × 0.05 = 0.75  
**想定ユースケース**: 連続運転が必須の設備、ダウンタイムコストが極めて高い場合

### 2. バランス型 (Balanced)
```python
Risk Weight: 1.0
Cost Lambda: 0.15
```

**コンセプト**: 安全とコストを両立した保全戦略  
**Replace実効コスト**: 15 × 0.15 = 2.25  
**想定ユースケース**: 標準的な設備保全、リスクとコストのバランス重視

### 3. コスト重視 (Cost Efficient)
```python
Risk Weight: 0.3
Cost Lambda: 0.5
```

**コンセプト**: 設備中断を許容し、必要最小限の保全  
**Replace実効コスト**: 15 × 0.5 = 7.5  
**想定ユースケース**: バックアップ可能な設備、コスト削減優先の場合

---

## 📈 実験結果

### シナリオ比較の可視化

![シナリオ比較](../outputs_comparison/scenario_comparison.png)

**図1**: 3つのシナリオの学習曲線と最終性能の比較。バランス型が圧倒的に優秀な性能を示している。

---

### 定量的パフォーマンス

| シナリオ | 平均報酬 | 最終100平均 | 最大報酬 | 標準偏差 | 訓練時間 |
|---------|---------|------------|---------|---------|---------|
| **🏆 バランス型** | **26.36** | **24.31** | **55.00** | 26.71 | 2.04分 |
| 安全重視 | 5.35 | 8.45 | 25.00 | 37.38 | 2.20分 |
| コスト重視 | -134.25 | -129.31 | -117.30 | 17.60 | 2.03分 |

### 学習曲線の特徴

#### バランス型（最優秀）

![バランス型詳細](../outputs_comparison/balanced_detailed.png)

**図2**: バランス型シナリオの詳細分析。4つのサブプロット（生報酬、移動平均、分布、累積平均）で学習過程を可視化。

**特徴:**
- 📈 Episode 100で報酬7.67 → Episode 500で31.24に急上昇
- 🎯 Episode 200-600で安定した高報酬（27-31）を維持
- ✅ 学習が早く、収束が安定
- 📊 最終100エピソードで24.31を達成
- 📉 標準偏差26.71で適度に安定
- 🎲 報酬分布が正の領域に集中

#### 安全重視（中程度の性能）

![安全重視詳細](../outputs_comparison/safety_first_detailed.png)

**図3**: 安全重視シナリオの詳細分析。Episode 500付近での急落と回復が特徴的。

**特徴:**
- 📈 Episode 100-400で11-13の中程度の報酬
- ⚠️ Episode 500で-23.83に急落（過学習の兆候）
- 🔄 Episode 700以降に回復傾向
- 📊 最終的に8.45で収束（変動大）
- 📊 標準偏差37.38と非常に不安定
- 🎢 報酬分布が広範囲に分散

#### コスト重視（完全失敗）

![コスト重視詳細](../outputs_comparison/cost_efficient_detailed.png)

**図4**: コスト重視シナリオの詳細分析。全エピソードでマイナス報酬、学習の失敗が明確。

**特徴:**
- ❌ 全エピソードで大幅なマイナス報酬（-127 ~ -150）
- 📉 改善の兆候なし、学習が機能していない
- 🚫 リスクペナルティが低すぎて保全行動を学習せず
- 💥 異常状態で継続運転による大ペナルティ
- 📊 標準偏差17.60と比較的小さいが、常にマイナス
- 🎯 報酬分布が完全にマイナス領域に集中

---

## 🎯 重要な発見

### 1. リスク-コストバランスの臨界点

**発見**: Cost Lambda 0.15が最適バランスポイント

- **Lambda < 0.1**: コストが低すぎ → 過剰保全 → 最適化不足
- **Lambda 0.15**: 適切なバランス → 最高性能
- **Lambda > 0.3**: コストが高すぎ → 保全不足 → 大きなリスクペナルティ

**数式的解釈**:
```
Total Reward = Risk Reward × Risk Weight - Action Cost × Cost Lambda

最適条件:
Risk Weight × |Risk Penalty| ≈ Action Cost × Cost Lambda × Expected Action Frequency
```

### 2. Risk Weightの重要性

**重要な教訓**: Risk Weightを下げすぎると学習が失敗する

- Risk Weight 1.0: 正常+1, 異常-10
- Risk Weight 0.3: 正常+0.3, 異常-3

コスト重視シナリオでは:
- 異常状態ペナルティ: -3
- Replace コスト: -7.5
- **結果**: 「何もしない」方が報酬が高い → 保全行動を学習しない

### 3. 環境特性との相互作用

**環境の特徴**:
- 異常状態持続率: 92.7%（非常に高い）
- 正常状態からの悪化率: 70.5%

**影響**:
- 積極的な保全が必要
- DoNothingでは異常状態に陥りやすい
- RepairよりReplaceが効果的（高い復旧率）

→ これが「バランス型」が優秀な理由

### 4. 過学習のリスク

**安全重視シナリオの問題**:
- Episode 500での急落 (-23.83)
- 標準偏差が大きい (37.38)

**原因仮説**:
- コストペナルティが低すぎ（0.05）
- エージェントが「Replace連打」のような極端な戦略を学習
- 探索と活用のバランスが崩れた

**教訓**: 適切なコストペナルティは過学習防止にも寄与

---

## 💡 実用的教訓

### Lesson 1: パラメータ調整の指針

**推奨設定（2x2 Markov CBM）**:
```python
Risk Weight: 1.0
Cost Lambda: 0.10 ~ 0.20
Replace Cost: 10 ~ 20
```

**調整方法**:
1. まずRisk Weight = 1.0で固定
2. Cost Lambdaを0.1から開始
3. 訓練を観察:
   - 報酬がマイナス → Lambdaを下げる（0.05刻み）
   - 変動が大きい → Lambdaを上げる（0.05刻み）
4. 0.10-0.20の範囲で最適値を探索

### Lesson 2: シナリオ選択のガイドライン

**設備の運用条件に応じて選択**:

| 条件 | 推奨Lambda | 期待性能 |
|------|-----------|---------|
| ダウンタイムコスト極大 | 0.08-0.12 | 中〜高 |
| 標準的な重要設備 | 0.12-0.18 | 最高 |
| バックアップ可能 | 0.15-0.25 | 中 |
| コスト最優先 | ❌ 不適合 | 低 |

**警告**: Cost Lambda > 0.3 または Risk Weight < 0.5 は推奨しない

### Lesson 3: 訓練の早期停止判断

**良好な学習の兆候**:
- ✅ Episode 200までに正の報酬を達成
- ✅ Episode 500までに報酬が20以上
- ✅ 標準偏差 < 30

**問題がある学習の兆候**:
- ❌ Episode 300以降もマイナス報酬
- ❌ 報酬の急激な変動（±50以上）
- ❌ 標準偏差 > 40

→ 問題がある場合はパラメータを再調整

### Lesson 4: 実装上の注意点

**1. Action Costのスケーリング**:
```python
# 推奨
ACTION_COSTS = [0, 3, 15]  # DoNothing, Repair, Replace
# Repairの3倍でReplace（現実的）
```

**2. Risk Penaltyのマグニチュード**:
```python
# 推奨
Normal: +1
Anomalous: -10
# 10倍の差でリスクの重大性を表現
```

**3. Horizon設定**:
```python
# 推奨: 100 steps
# 理由: 長期的な保全効果を評価可能
```

---

## 🔍 詳細分析

### バランス型が優秀な理由

**1. 適切なコスト-ベネフィット比**:
```
Replace実効コスト: 2.25
異常状態10ステップの損失: 10 × 10 = 100
→ Replace が圧倒的に有利

一方、コスト重視では:
Replace実効コスト: 7.5
異常状態10ステップの損失: 3 × 10 = 30
→ コストとペナルティが拮抗（保全意欲低）
```

**2. 効果的な探索-活用バランス**:
- Noisy Networksによる探索
- 適度なコストペナルティによる活用促進
- PERによる重要経験の優先学習

**3. 環境特性へのマッチング**:
- 高異常持続率(92.7%)に対応
- Replace行動で効果的に正常状態に復帰
- 長期的利益を正しく評価

### 安全重視の不安定性

**Episode 500での急落原因**:

可能性1: 局所最適への過学習
```python
# 極端な戦略例
if state == Anomalous:
    action = Replace  # 常にReplace
# コストが低いため短期的には高報酬
# しかし長期的に非効率
```

可能性2: Noisy Networksの影響
```python
# Episode 500前後でノイズの影響が変化
# 新しい探索が悪い結果を生む
```

**対策**:
- Cost Lambdaを0.08-0.12に調整
- Target Network同期頻度を調整
- Epsilon-greedyとの併用を検討

### コスト重視の完全失敗

**失敗メカニズム**:

1. 初期エピソード:
   - Agent: "Replace高すぎ（コスト7.5）"
   - Agent: "DoNothingで様子見"

2. 結果:
   - 異常状態に陥る（70.5%の確率）
   - 異常状態が継続（92.7%の確率）
   - 累積ペナルティ: -3 × 長時間

3. 学習:
   - "Replaceは避けるべき"を強化学習
   - 悪循環に陥る

**数値例**:
```
Episode初期に異常状態へ遷移
→ DoNothingを90ステップ継続
→ 報酬: -3 × 90 = -270

Replaceを実行した場合
→ コスト: -7.5
→ 正常状態で90ステップ
→ 報酬: -7.5 + 0.3 × 90 = +19.5

しかしエージェントは短期的なコストを過大評価
→ 学習失敗
```

---

## 📚 理論的考察

### QR-DQNの有効性

**分位点回帰の利点**:
- リスクの分布を学習
- VaR, CVaRによるリスク評価
- 不確実性を考慮した意思決定

**本実験での観察**:
- バランス型: 分位点分布が収束
- コスト重視: 分位点がマイナス領域に集中
- 安全重視: 分位点の分散が大きい

### N-step Learningの効果

**長期的視点の重要性**:
```
n=3を使用:
Replace実行 → 3ステップ先までの報酬を考慮
→ 短期コストより長期利益を重視
```

→ バランス型で特に効果的

### Prioritized Experience Replayの貢献

**重要な経験の優先学習**:
- 異常→正常遷移: 高優先度
- Replace成功: 高優先度
- 累積ペナルティ: 学習重点化

→ 効率的な学習に寄与

---

## 🎓 まとめと推奨事項

### 重要な結論

1. **バランス型（Lambda=0.15）が最適**
   - 最高報酬: 24.31
   - 安定性: 標準偏差 26.71
   - 汎用性: 多様な設備に適用可能

2. **リスク-コストバランスは環境依存**
   - 異常持続率が高い → より積極的な保全必要
   - 正常維持率が高い → 穏健な保全で可

3. **過度なコスト削減は逆効果**
   - コスト重視シナリオは完全失敗
   - 適切なコストペナルティが学習を促進

### 実装推奨事項

**1. 初期設定**:
```python
env = EquipmentCBMEnvironment(
    scenario='balanced',  # まずはこれで試す
    horizon=100,
    gamma=0.95
)
```

**2. パラメータチューニング**:
```python
# 環境に応じて調整
if 高ダウンタイムコスト:
    cost_lambda = 0.10
elif 標準設備:
    cost_lambda = 0.15
elif 低重要度設備:
    cost_lambda = 0.20
```

**3. 訓練設定**:
```python
# 推奨
episodes = 1000  # 十分な学習
n_envs = 16      # 高速化
n_quantiles = 51 # QR-DQN
```

**4. 評価基準**:
- 最終100エピソード平均 > 20: 優秀
- 最終100エピソード平均 > 10: 良好
- 最終100エピソード平均 > 0: 許容
- 最終100エピソード平均 < 0: 要パラメータ調整

### 今後の研究課題

1. **動的パラメータ調整**:
   - 訓練中にCost Lambdaを自動調整
   - カリキュラム学習の適用

2. **マルチシナリオ学習**:
   - 複数シナリオで訓練したユニバーサルエージェント
   - Meta-Learning適用

3. **リアルタイム適応**:
   - 運用中の環境変化に対応
   - オンライン学習の実装

4. **説明可能性の向上**:
   - 意思決定理由の可視化
   - 操作員への提案システム

---

## 📊 参考データ

### 訓練時間比較

- **バランス型**: 122.15秒 (2.04分) - 0.122秒/エピソード
- **安全重視**: 131.99秒 (2.20分) - 0.132秒/エピソード  
- **コスト重視**: 121.96秒 (2.03分) - 0.122秒/エピソード

→ パラメータによる訓練速度差は小さい（並列化の効果）

### リソース使用量

- **GPU使用**: CUDA有効
- **並列環境数**: 16
- **メモリ使用**: 約2GB
- **CPU使用率**: 40-60%（並列環境処理）

---

## 🔗 関連ファイル

### コードファイル
- `cbm_environment.py`: 環境実装（2x2 Markov CBM with scenario support）
- `train_cbm_dqn_v2.py`: 訓練スクリプト（QR-DQN with full optimizations）
- `compare_scenarios.py`: シナリオ比較実行（訓練 + 評価）
- `visualize_scenarios.py`: シナリオ可視化専用スクリプト

### 出力ディレクトリ
- `outputs_balanced/`: バランス型結果（1000 episodes）
- `outputs_safety_first/`: 安全重視結果（1000 episodes）
- `outputs_cost_efficient/`: コスト重視結果（1000 episodes）

### 可視化ファイル
- `outputs_comparison/scenario_comparison.png`: シナリオ比較グラフ（学習曲線 + 最終性能）
- `outputs_comparison/balanced_detailed.png`: バランス型詳細分析（2×2サブプロット）
- `outputs_comparison/safety_first_detailed.png`: 安全重視詳細分析（2×2サブプロット）
- `outputs_comparison/cost_efficient_detailed.png`: コスト重視詳細分析（2×2サブプロット）

---

**作成日**: 2025年12月21日  
**バージョン**: v2.0  
**実験環境**: Equipment CBM QR-DQN with full optimizations
